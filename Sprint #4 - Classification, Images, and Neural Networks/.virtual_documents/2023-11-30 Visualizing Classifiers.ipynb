%matplotlib inline
from pylab import *


from classy import *


images=image.load_images('images/training squares/')
images=remap_targets(images,new_target_names=['blank','player1','player2'])
summary(images)


images['data'][0].shape


data=image.images_to_vectors(images)


data_train,data_test=split(data,test_size=0.2)





C=NaiveBayes()
C.fit(data_train.vectors,data_train.targets)
print("On Training Set:",C.percent_correct(data_train.vectors,data_train.targets))
print("On Test Set:",C.percent_correct(data_test.vectors,data_test.targets))


C.means


C.means.shape


mean0=C.means[0,:]


mean0.shape


im0=mean0.reshape((50,50,3))
im0=im0-im0.min()  # set the min to zero
im0=im0/im0.max()  # set the max to 1


imshow(im0)


for i in range(3):
    subplot(1,3,i+1)
    mean0=C.means[i,:]
    im0=mean0.reshape((50,50,3))
    im0=im0-im0.min()  # set the min to zero
    im0=im0/im0.max()  # set the max to 1
    imshow(im0)





C=CSC()
C.fit(data_train.vectors,data_train.targets)
print("On Training Set:",C.percent_correct(data_train.vectors,data_train.targets))
print("On Test Set:",C.percent_correct(data_test.vectors,data_test.targets))


C.centers.shape


C.targets


for i in range(18):  # make sure this matches the shape
    subplot(3,6,i+1) # make sure that the 3 and 6 multiply to be greater than or equal to the number of centers

    mean0=C.centers[i,:]
    im0=mean0.reshape((50,50,3))
    im0=im0-im0.min()  # set the min to zero
    im0=im0/im0.max()  # set the max to 1
    imshow(im0) 
    title(C.targets[i])

















standardize(data)
summary(data)


data_train,data_test=split(data,test_size=0.2)


data_train.vectors.shape


number_of_features=data_train.vectors.shape[1]
number_of_categories=3  # the types of pieces


C=NumPyNetBackProp({
    'input':number_of_features,               # number of features
    'output':(number_of_categories,'linear'),  # number of classes
    'cost':'mse',
})



C.fit(data_train.vectors,data_train.targets,epochs=5000)   # you'll want to increase epochs here


print(("On Training Set:",C.percent_correct(data_train.vectors,data_train.targets)))
print(("On Test Set:",C.percent_correct(data_test.vectors,data_test.targets)))


len(C.weights)


C.weights[0]  # first layer


W=C.weights[0]
W.shape


for i in range(3):
    subplot(1,3,i+1)
    vec=W[:,i]
    vec=(vec-W.min())/(W.max()-W.min())  # rescale to 0-1
    im=vec.reshape((50,50,3))
    imshow(im)





X=rand(20000,7500)


C.output(X)[0].shape


y=C.output(X)[0]


output_images=[]
for v in y.T:
    v=atleast_2d(v)
    im_vec=(v.T*X).sum(axis=0)
    im_vec=(im_vec-im_vec.min())/(im_vec.max()-im_vec.min()) 
    im=im_vec.reshape(50,50,3)
    output_images.append(im)


for i in range(3):
    subplot(1,3,i+1)
    imshow(output_images[i])











C=NumPyNetBackProp({
    'input':number_of_features,               # number of features
    'hidden':[(6,'logistic'),],   # this size is "arbitrary"
    'output':(number_of_categories,'logistic'),  # number of classes
    'cost':'mse',
})


C.fit(data_train.vectors,data_train.targets,epochs=500)   # you'll want to increase epochs here


print(("On Training Set:",C.percent_correct(data_train.vectors,data_train.targets)))
print(("On Test Set:",C.percent_correct(data_test.vectors,data_test.targets)))


len(C.weights)


W=C.weights[0]
W.shape


W=C.weights[1]
W.shape


W=C.weights[0]
for i in range(4):
    subplot(2,3,i+1)
    vec=W[:,i]
    vec=(vec-W.min())/(W.max()-W.min())  # rescale to 0-1
    im=vec.reshape((50,50,3))
    imshow(im)





X=rand(20000,7500)
y=C.output(X)


y[0].shape


for i in range(11):
    pass
print(i)


from tqdm import tqdm


def reverse_correlation(C,N=20000):
    X=rand(N,7500)
    y=C.output(X)

    all_ims=[]
    for l in range(len(y)):    
        i=0
        ims=[]
        for v in y[l].T:
            v=atleast_2d(v)
            im_vec=(v.T*X).sum(axis=0)/N
            #im_vec=(im_vec-im_vec.min())/(im_vec.max()-im_vec.min()) 
            im=im_vec.reshape(50,50,3)

            ims.append(im)
    
            i+=1  
        all_ims.append(ims)

    return all_ims


all_ims=reverse_correlation(C)


for k,ims in enumerate(all_ims):
    for i,im in enumerate(ims):
        print(all_ims[k][i].min(),all_ims[k][i].max())



num_repeat=20  # 60 
for repeat in tqdm(range(num_repeat)):
    all_ims2=reverse_correlation(C)

    for k,ims in enumerate(all_ims2):
        for i,im in enumerate(ims):
            all_ims[k][i]+=im    



for ims in all_ims:
    figure()
    n=len(ims) 
    c=int(ceil(sqrt(n)))
    r=n//c
    if r*c<n:
        r+=1

    
    for i,im in enumerate(ims):
        subplot(r,c,i+1)

        im=(im-im.min())/(im.max()-im.min()) 
        imshow(im)



